---
layout: post
title:  "Deep Learning-word2vec"
date:   2016-10-23 10:43:16 +0800
categories: Deep Learning
#header-img: "img/post-bg-js-module.jpg"
tags:
    - DeepLearning 
    - word2vec 
    - è¯å‘é‡
---

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


## è¯å‘é‡

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œéœ€è¦å°†å…·ä½“çš„æ–‡å­—æ•°å­—åŒ–ï¼Œé‚£ä¹ˆå°±å¯ä»¥å½“åšå¸¸è§„çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚

æ¯”å¦‚çŸ­æ–‡æœ¬åˆ†ç±»ï¼Œé¦–å…ˆæ˜¯ä¸€ä¸ªå¥å­çš„å½¢å¼ï¼Œéœ€è¦è½¬åŒ–ä¸ºå•ä¸ªè¯è¯­ã€‚ç„¶åè¿˜éœ€è¦è½¬åŒ–ä¸ºå…·ä½“æ•°å­—ï¼Œç”¨æ¥æè¿°ã€åˆ†æè¯ä¹‹é—´çš„å…³ç³»ç­‰ç­‰ã€‚

**One-hot Representation**

æ¯”å¦‚

```
è‹¹æœ [0 0 0 1 0 0 0 0 ...]
é¦™è•‰ [0 0 0 0 0 0 1 0 ...]
```
é‚£ä¹ˆæ¯ä¸ªè¯å°±æ˜¯ä¸€å †0çš„ä¸­çš„1.
è¿™ç§ç§°ä¸º **One-hot Representation**ï¼Œä¸€ç§ç¼–ç æ–¹å¼ã€‚é‚£ä¹ˆæ•°å­—åŒ–ä¹‹åï¼Œå°±å¯ä»¥ä½œè®¡ç®—ã€‚

ä¸è¿‡è¿™ç§è¡¨ç¤ºæ–¹å¼ï¼Œè¯è¡¨çš„çŸ©é˜µæ˜¯ä¸ªç¨€ç–çŸ©é˜µï¼Œå¦‚æœè¯æ±‡é‡å¾ˆå¤§ï¼Œé‚£ä¹ˆä¼šæ˜¯ä¸€ä¸ªå¾ˆå¤§çš„ç»´åº¦ï¼Œä½œå­˜å‚¨å’Œè®¡ç®—éƒ½ä¸æ˜¯å¾ˆç†æƒ³ï¼›è€Œä¸”å¯¹äºè¯ä¹‹é—´çš„å…³ç³»ä¹Ÿæ— æ³•ä½“ç°ã€‚é‚£ä¹ˆå­˜åœ¨å¦ä¸€ç§æ–¹å¼

**Distributed Representation**

ä¸One-hot Representationæƒ³æ³•ç±»ä¼¼ï¼Œä¸è¿‡æ›´ä¸ºç²¾ç®€ï¼Œäººä»¬å¸Œæœ›å¯ä»¥è¿™æ ·ï¼š

```
[0.302 0.123 0.690 ...] 

```
ä¸€èˆ¬åªæœ‰50ï¼Œ100ç»´åº¦ã€‚å°±è¶³ä»¥è¡¨è¾¾æˆåƒä¸Šä¸‡è¯æ±‡ã€‚å¹¶ä¸”è¿˜èƒ½è®¡ç®—è¯ä¹‹é—´çš„å…³ç³»ã€‚æ¯”å¦‚ è‹¹æœï¼Œé¦™è•‰ã€‚åº”è¯¥å¾ˆæ¥è¿‘æ‰æ˜¯ï¼Œå› ä¸ºéƒ½æ˜¯æ°´æœã€‚word2vecå°±å¯ä»¥è®­ç»ƒè¿™æ ·çš„è¯å‘é‡ã€‚

**word2vec**

word2vecæ˜¯Googleå¼€æºçš„ä¸€ä¸ªé«˜æ•ˆè®¡ç®—è¯å‘é‡çš„å·¥å…·ã€‚è®¡ç®—å¾—åˆ°çš„è¯å‘é‡å¯åº”ç”¨äºå„ç§NLPä»»åŠ¡ã€‚å…¶é«˜æ•ˆåœ¨äºä½¿ç”¨äº†2ç§æ¨¡å¼cbowå’Œskip-gram.


## NNLM

word2vecä½œä¸ºä¸€ç§ç®€åŒ–çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚åœ¨äº†è§£å†…éƒ¨åŸç†ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“ä¸‹ NNLMç½‘ç»œç»“æ„ã€‚

NNLM å³ Neural Network Language Modelï¼Œç”±Bengioæå‡ºã€ŠA Neural Probabilistic Language Modelã€‹

![](https://raw.githubusercontent.com/y521263/y521263.github.io/master/img/article/2016-10-23-nnlm.png)


å›¾ä¸­ï¼Œè¯¥ç½‘ç»œåˆ†2å±‚ï¼Œä»ä½å¾€ä¸Šï¼Œè¾“å…¥å±‚ï¼Œéšè—å±‚ï¼Œè¾“å‡ºå±‚ã€‚

$$w_{t-n+1},...w_{t-2},w_{t-1}$$è¡¨ç¤ºå‰é¢n-1ä¸ªè¯ï¼Œæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯$$w_t$$.

å›¾ä¸­æœ‰ä¸ª **C**,å…¶å®å°±æ˜¯è¯æ±‡è¡¨ï¼Œå­˜æ”¾ç€ç›¸åº”çš„è¯å‘é‡ã€‚$$C(W)$$å°±æ˜¯å¯¹åº”çš„è¯å‘é‡ã€‚è¿™é‡Œè¾“å…¥çš„è¯å‘é‡å…¶å®æ˜¯ä¸ªindexã€‚

ç¬¬ä¸€å±‚ï¼Œn-1ä¸ªè¯$$C(w_{t-n+1}),...C(w_{t-2}),C(w_{t-1})$$ï¼Œé¦–å°¾ç›¸è¿ï¼Œå½¢æˆ(n-1)mç»´çš„å‘é‡ã€‚mä¸ºå•ä¸ªè¯å‘é‡ç»´åº¦ï¼Œæ¯”å¦‚100ã€‚

ç¬¬äºŒå±‚ï¼Œä¹Ÿå°±æ˜¯éšè—å±‚ï¼Œ$$d+Hx$$è®¡ç®—å¾—åˆ°ã€‚dæ˜¯åç½®é¡¹ï¼ŒHæ˜¯éšè—å±‚å‚æ•°($$H=h(n-1)m$$).xå°±æ˜¯ç¬¬ä¸€å±‚çš„è¾“å…¥ã€‚

ç¬¬ä¸‰å±‚ï¼Œæœ‰$$V$$ä¸ªèŠ‚ç‚¹ï¼Œä¹Ÿå°±æ˜¯è¯æ±‡è¡¨$$C(V*m)$$çš„å¤§å°ï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡º$$y_i$$è¡¨ç¤ºä¸‹ä¸€ä¸ªè¯æ˜¯içš„æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯æ€»å…±æœ‰Vä¸ªæ¦‚ç‡ï¼Œå–æœ€å¤§çš„æ¦‚ç‡å’¯ã€‚è¿™é‡Œçš„yæ˜¯æœªå½’ä¸€åŒ–çš„logæ¦‚ç‡.

 $$y=b+Wx+Utanh(d+Hx)$$

 $$W(VÃ—(n-1)m)$$ ä¹Ÿå°±æ˜¯ä¸Šå›¾ä¸­å·¦è¾¹çš„è™šçº¿ï¼Œè¾“å…¥å±‚åˆ°è¾“å‡ºå±‚çš„ç›´è¿ã€‚å¦‚æœå»æ‰ç›´è¿ï¼Œé‚£ä¹ˆWå‚æ•°å°±ä¸º0äº†ã€‚

 $$U(VÃ—h)$$å°±æ˜¯éšè—å±‚åˆ°è¾“å‡ºå±‚çš„å‚æ•°

 $$b(V)$$ biasé¡¹

é‚£ä¹ˆè¿™é‡Œæ‰€æœ‰çš„å‚æ•°$$\theta$$

 $$\theta=(b,d,W,U,H,C)$$

åˆ†æå®Œæˆï¼Œé‚£ä¹ˆæ¥ä¸‹æ¥å°±æ˜¯ä¸€ä¸ªæ±‚è§£ç¥ç»ç½‘ç»œçš„é—®é¢˜ï¼Œè®ºæ–‡ä¸­ç”¨çš„æ˜¯éšæœºæ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–å‡ºæ¥ã€‚è¿™é‡Œæœ‰ç‚¹ç‰¹åˆ«çš„æ˜¯ï¼Œè¾“å…¥å±‚å‚æ•°Cï¼Œä¹Ÿå°±æ˜¯è¯å‘é‡ï¼Œä¹Ÿæ˜¯å‚æ•°ï¼Œä¹Ÿè¦æ±‚è§£çš„ï¼Œä¸ä¸€èˆ¬çš„ç¥ç»ç½‘ç»œä¸åŒçš„åœ°æ–¹ã€‚
è§£å‡ºæ¥ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°2æ ·ä¸œè¥¿ï¼Œä¸€ä¸ªCï¼Œä¹Ÿå°±æ˜¯è¯å‘é‡ï¼Œè¿˜æœ‰ä¸€ä¸ªæ˜¯è¯­è¨€æ¨¡å‹ã€‚æœ¬æ–‡æˆ‘ä»¬åªè°ˆè¯å‘é‡ã€‚




## word2vec

åœ¨NNLMä¸­è®¡ç®—çš„å¼€é”€ä¸»è¦åœ¨äºéšè—å±‚ï¼Œä»¥åŠå› æ­¤å±‚åˆ°è¾“å‡ºå±‚çš„å‚æ•°è®¡ç®—ã€‚
åœ¨word2vecæ‰€ä»¥å¹²è„†å»æ‰äº†éšå±‚ã€‚å¤§å¤§æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œå¹¶æå‡ºäº†2ç§æ¨¡å‹CBOWå’Œskip-gram

### CBOW

CBOWæ˜¯Continuous Bag-of-Words Modelã€‚ä¸NNLMæ¨¡å‹ç±»ä¼¼ï¼Œä¸è¿‡å»æ‰äº†éšè—å±‚ã€‚

![](https://raw.githubusercontent.com/y521263/y521263.github.io/master/img/article/2016-10-23-cbow.png)

ç”±å›¾å¯ä»¥çœ‹å‡ºæ¥ï¼Œè¿™é‡Œçš„è¾“å…¥æ˜¯ç›´æ¥æ±‚å’Œåˆ°éšè—å±‚ã€‚è¦æ±‚çš„æ˜¯

$$P(w_t|w_{t-k},w_{t-k+1},...w_{t+k},)$$

ä¸NNLMä¸åŒçš„åœ°æ–¹

* åœ¨NNLMä¸­æ˜¯è¯å‘é‡é¦–å°¾ç›¸è¿å˜æˆå¾ˆé•¿çš„ä¸€ä¸²ï¼Œè¿™é‡Œæ˜¯ç›´æ¥æ±‚å’Œ/æ±‚å¹³å‡

* è¿™é‡Œçš„æ±‚tï¼Œç”¨äº†tå‰åçš„è¯å‘é‡ï¼Œè€Œå‰é¢æ˜¯å‰n-1ä¸ªè¯ã€‚ä¼¼ä¹çœ‹èµ·æ¥è¿™æ ·æ›´åˆç†ï¼Œå› ä¸ºè€ƒè™‘äº†å°†æ¥çš„è¯ã€‚

* CBOWå»æ‰äº†éšè—å±‚å¤æ‚çš„è®¡ç®—ã€‚

inutåˆ°projectå±‚ï¼Œæ¥çœ‹ä¸‹æºä»£ç ï¼Œæˆ‘åŠ äº†æ³¨é‡Šã€‚

``` c++
	  //éšæœºé€‰æ‹©çª—å£å¤§å°
     b = next_random % window;
     if (cbow) {  //train the cbow architecture
      // in -> hidden
      cw = 0;
      for (a = b; a < window * 2 + 1 - b; a++) if (a != window) {
        //ä»¥ sentence_position ä¸ºä¸­å¿ƒ å‰åå„ window - b ä¸ªè¯
        c = sentence_position - window + a;
        if (c < 0) continue;
        if (c >= sentence_length) continue;
        last_word = sen[c];
        if (last_word == -1) continue;
        //çª—å£å†…è¯å‘é‡ç›¸åŠ 
        for (c = 0; c < layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];
        cw++;
      }
	  if (cw) {
        //ç›¸åŠ çš„è¯å‘é‡å–å¹³å‡
        for (c = 0; c < layer1_size; c++) neu1[c] /= cw;
```

é‚£ä¹ˆæ¥ä¸‹æ¥å°±æ˜¯projectå±‚åˆ°è¾“å‡ºå±‚çš„è®¡ç®—ã€‚è¿™é‡Œåˆæœ‰2ç§æ–¹å¼ï¼Œ **Hierarchical Softmax**å’Œ **NEGATIVE SAMPLING**ã€‚


**Hierarchical Softmax**

å…³äºè¿™ä¸ªæ€æƒ³çš„æå‡ºï¼Œæˆ‘ä»¬æƒ³ä¸‹ä¹‹å‰çš„NNLMä¸­çš„è¾“å‡ºæœ‰Vä¸ªèŠ‚ç‚¹ï¼Œå¯¹äºæ¯ä¸ªèŠ‚ç‚¹éƒ½è¦è®¡ç®—å…¶æ˜¯ä¸æ˜¯çš„æ¦‚ç‡å€¼ï¼Œé‚£ä¹ˆè®¡ç®—é‡è¿˜æ˜¯æ¯”è¾ƒå¤§çš„ã€‚

å¦‚æœæ¢ä¸ªæ€è·¯ï¼Œæˆ‘ä»¬å…ˆæŠŠè¯è¯­åˆ†ä¸ªç±»ï¼Œåšä¸ªç¼–ç ã€‚æ¯”å¦‚ï¼Œè‹¹æœã€‚å…ˆåˆ¤æ–­æ˜¯ä¸æ˜¯æ°´æœï¼Œå†åˆ¤æ–­æ˜¯ä¸æ˜¯è‹¹æœã€‚è®ºæ–‡ä¸­çš„æ–¹æ³•ï¼Œæ˜¯å…ˆå¯¹æ‰€æœ‰è¯ä½œHuffmanç¼–ç ã€‚é‚£ä¹ˆé«˜é¢‘è¯çš„ç¼–ç å°±å¾ˆçŸ­ã€‚æ‰€æœ‰çš„è¯éƒ½æ˜¯Huffmanæ ‘çš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚

å‡å¦‚$$w$$=è‹¹æœã€‚å…¶ç¼–ç æ˜¯1010.é‚£ä¹ˆ$L(w)=5$ï¼Œç®—ä¸Šrootæ ¹èŠ‚ç‚¹ã€‚è¡¨ç¤ºè¯¥è¯æ‰€åœ¨èŠ‚ç‚¹åˆ°rootæ ¹èŠ‚ç‚¹çš„è·ç¦»æ˜¯4ã€‚$$n(w,j)$$è¡¨ç¤ºrooté€šå¾€wçš„ç¬¬jä¸ªèŠ‚ç‚¹ï¼Œ$$n(w,1)$$=root,
$$n(w,L(w))=w$$

è¾“å‡ºï¼š

$$P(w_O|w_I)=\Pi_{j=1}^{L(w)-1}\sigma([n(w,j+1)=ch(n(w,j))]*{v_{n(w,j)}^{'}}^{T} v_{wI} )$$

è¿™é‡Œçš„$$\sigma(x)=\frac{1}{1+e^{-x}}$$

$$[n(w,j+1)=ch(n(w,j))]$$è¡¨ç¤º$$n(w,j+1)$$æ˜¯$$n(w,j)$$çš„ä¸€ä¸ªå­èŠ‚ç‚¹ï¼Œæ„Ÿè§‰è¿™æ˜¯å¾ˆæ˜¾ç„¶çš„äº‹ï¼Œåˆå¤šè¿™ä¹ˆä¸€ä¸ªæ¦‚å¿µã€‚

[x]è¡¨ç¤ºx=true åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º-1ã€‚

$$v_n$$è¡¨ç¤ºHuffmanæ ‘çš„inner(å†…éƒ¨)èŠ‚ç‚¹ä¹Ÿå°±æ˜¯éå¶å­èŠ‚ç‚¹ï¼›

$$v_w$$è¡¨ç¤ºHuffmanæ ‘çš„å¶å­èŠ‚ç‚¹ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªä¸ªè¯ã€‚

é‚£ä¹ˆ å…·ä½“å¦‚ä½•æ±‚è§£ï¼Œæˆ‘ä»¬çœ‹ä¸‹ä»£ç 

``` c++
		//Hierarchical Softmax ä¹Ÿå°±æ˜¯æ ¹æ®åˆå§‹åŒ–çš„ vocab è¯æ±‡è¡¨çš„huffmanç¼–ç æ¥è®¡ç®—è¾“å‡ºfå€¼ï¼Œ
        if (hs) for (d = 0; d < vocab[word].codelen; d++) {
          f = 0;
          l2 = vocab[word].point[d] * layer1_size;
          // Propagate hidden -> output
          for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1[c + l2];
          if (f <= -MAX_EXP) continue;
          else if (f >= MAX_EXP) continue;
          //få€¼è½¬åŒ–åˆ°0.01ï½1 ä¹‹é—´ æŸ¥è¡¨expTableå¾—åˆ°ï¼ŒexpTableçš„é»˜è®¤å¤§å°ä¸ºEXP_TABLE_SIZEï¼1000
          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
          // 'g' is the gradient multiplied by the learning rate
          // 1 - vocab[word].code[d] è¡¨ç¤ºwordçš„ç¬¬dä½huffmanç¼–ç  0æˆ–è€…1 ç”¨æ¥è¡¨ç¤ºç›®æ ‡å€¼ï¼Œå‡å»è¾“å‡ºf è®¡ç®—g
          g = (1 - vocab[word].code[d] - f) * alpha;
          // Propagate errors output -> hidden
          //g * syn1[c + l2] æ¢¯åº¦éƒ¨åˆ†,è¯¯å·®å›ä¼  éšå±‚ã€‚åœ¨åé¢ neu1eä¼šç»§ç»­å›ä¼ åˆ°è¾“å…¥å±‚ï¼Œç”¨æ¥ä¼˜åŒ–syn0
          for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];
          // Learn weights hidden -> output
          for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * neu1[c];

```

è¿™é‡Œçš„syn1å…¶å®å°±æ˜¯ä¸Šé¢æåˆ°çš„Huffmançš„inner(å†…éƒ¨)èŠ‚ç‚¹ã€‚

$$f=\sigma(neu1^Tsyn1)$$

æ¢¯åº¦

$$g = (1 - vocab[word].code[d] - f) * alpha$$


1 - vocab[word].code[d] è¡¨ç¤ºwordçš„ç¬¬dä½huffmanç¼–ç  0æˆ–è€…1,æ¯”å¦‚è‹¹æœçš„ç¬¬0ä½æ˜¯1.


**NEGATIVE SAMPLING**


å†è¯´ä¸€ä¸‹å¦ä¸€ç§æ–¹å¼ï¼ŒNEGATIVE SAMPLINGã€‚ä¸»è¦çš„æ€æƒ³æ˜¯ï¼Œéšæœºç”Ÿæˆä¸€äº›è´Ÿæ ·æœ¬ï¼Œå¦‚æœå‘½ä¸­ word åˆ™ label=1 ä¸æ›´æ–°è¯¯å·®ï¼›å¦åˆ™æ›´æ–°è¯¯å·® æ›´æ–°ç½‘ç»œå‚æ•°

ä»£ç ï¼š

```
		//è´Ÿé‡‡æ · (é»˜è®¤é‡‡ç”¨è¿™ç§æ–¹å¼)
        //éšæœºé€‰æ‹© negative ä¸ªè¯ï¼Œå¦‚æœå‘½ä¸­ word åˆ™ ä¸æ›´æ–°è¯¯å·®ï¼›å¦åˆ™æ›´æ–°è¯¯å·® æ›´æ–°ç½‘ç»œå‚æ•°
        // NEGATIVE SAMPLING
        if (negative > 0) for (d = 0; d < negative + 1; d++) {
          if (d == 0) {
            target = word;
            label = 1;
          } else {
            next_random = next_random * (unsigned long long)25214903917 + 11;
            target = table[(next_random >> 16) % table_size];
            if (target == 0) target = next_random % (vocab_size - 1) + 1;
            if (target == word) continue;
            label = 0;
          }
          l2 = target * layer1_size;
          f = 0;
          for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1neg[c + l2];
          if (f > MAX_EXP) g = (label - 1) * alpha;
          else if (f < -MAX_EXP) g = (label - 0) * alpha;
          //æŸ¥è¡¨
          else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;
          for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];
          for (c = 0; c < layer1_size; c++) syn1neg[c + l2] += g * neu1[c];
```

ä¸å‰é¢çš„å·®åˆ«åœ¨äº 

$$g = (label - f) * alpha$$

ç”¨labelä»£æ›¿ 1 - vocab[word].code[d].è€Œè¿™ä¸ªlabelæ˜¯éšæœºç”Ÿæˆçš„ï¼Œå½“ç„¶æ˜¯è€ƒè™‘äº†è¯é¢‘çš„éšæœºæŠ½æ ·ã€‚


### Skip-gram

Skip-gramçš„æ¨¡å‹å›¾ä¸cbowæ­£å¥½åä¸€ä¸‹ã€‚

![](https://raw.githubusercontent.com/y521263/y521263.github.io/master/img/article/2016-10-23-skip-gram.png)

ç”±å½“å‰è¯å»æ¨å‘¨å›´è¯çš„æ¦‚ç‡ã€‚

è§ä»£ç æ³¨é‡Šã€‚

``` c++
} else {  //train skip-gram
      for (a = b; a < window * 2 + 1 - b; a++) if (a != window) {
        c = sentence_position - window + a;
        if (c < 0) continue;
        if (c >= sentence_length) continue;
        last_word = sen[c];
        if (last_word == -1) continue;
        l1 = last_word * layer1_size;
        for (c = 0; c < layer1_size; c++) neu1e[c] = 0;
        // HIERARCHICAL SOFTMAX
        if (hs) for (d = 0; d < vocab[word].codelen; d++) {
          f = 0;
          l2 = vocab[word].point[d] * layer1_size;
          // Propagate hidden -> output
          // skip-gramä¸cbowä¹‹é—´çš„å·®åˆ«
          // è¿™é‡Œçš„f æ˜¯ç›´æ¥è¾“å…¥ syn0 ä¸è¾“å‡ºsyn1ç›¸ä¹˜
          // è€Œcbowä¸­ æ˜¯è¾“å…¥çš„å‘é‡ç›¸åŠ å–å¹³å‡åå†ä½œè®¡ç®—
          // skip-gramçš„ç†å¿µåœ¨äº ç”¨å½“å‰è¾“å…¥çš„è¯ word[t] å»æ¨ ä¸Šä¸‹æ–‡ word[t-2] word[t-1] word[t+1] word[t+2]çš„æ¦‚ç‡.
          // å…·ä½“çš„è®¡ç®—ç›®æ ‡å°±æ˜¯ä¼˜åŒ–è¾“å‡ºsyn1å¯¹åº”çš„ä¸Šä¸‹æ–‡è¯å‘é‡
          for (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];
          if (f <= -MAX_EXP) continue;
          else if (f >= MAX_EXP) continue;
          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
          // 'g' is the gradient multiplied by the learning rate
          g = (1 - vocab[word].code[d] - f) * alpha;
          // Propagate errors output -> hidden
          for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];
          // Learn weights hidden -> output
          for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * syn0[c + l1];
        }

```



### PS

çŸ­çŸ­700è¡Œçš„ä»£ç ï¼Œåšå®Œäº†æ‰€æœ‰çš„äº‹æƒ…ã€‚è™½ç„¶ä»£ç é£æ ¼å¾ˆè±ªæ”¾ï¼Œä¸€å †abcå˜é‡å<code>:P</code>

ä¸è¿‡ç¼–è¯‘è¿è¡Œè¿˜æ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œæ— ä»»ä½•ä¾èµ–äº†ï¼Œç”šè‡³è¿éšæœºæ•°ç”Ÿæˆéƒ½è‡ªå·±å†™ã€‚ã€‚ã€‚

è¿™é‡Œå†æ¨èå‡ ç¯‡æ–‡ç«  @licstarçš„[ã€ŠDeep Learning in NLP ï¼ˆä¸€ï¼‰è¯å‘é‡å’Œè¯­è¨€æ¨¡å‹ã€‹](http://licstar.net/archives/328)ï¼Œå…³äºè¯å‘é‡çš„å‰ä¸–ä»Šç”Ÿè®²çš„å¾ˆè¯¦ç»†ï¼›

è¿˜æœ‰ç½‘æ˜“çš„[Deep Learningå®æˆ˜ä¹‹word2vec](http://techblog.youdao.com/?p=915)æœ‰ä»£ç è®²è§£ï¼Œè¿˜æœ‰word2vecç›¸å…³äººç‰©çš„å…«å¦ğŸ˜„ï½


å®Œæ•´æ³¨é‡Šä»£ç [word2vec.c](https://github.com/y521263/AlgorithmSet)


### å‚è€ƒ

1.[word2vec](https://code.google.com/archive/p/word2vec/)

2.[A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

3.[Distributed Representations of Words and Phrasesand their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)

4.[Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)

5.[Deep Learning in NLP ï¼ˆä¸€ï¼‰è¯å‘é‡å’Œè¯­è¨€æ¨¡å‹](http://licstar.net/archives/328)

6.[Deep Learningå®æˆ˜ä¹‹word2vec](http://techblog.youdao.com/?p=915)